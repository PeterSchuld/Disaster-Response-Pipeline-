{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score,label_ranking_average_precision_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    '''\n",
    "    Load data from database\n",
    "    \n",
    "    Output:\n",
    "    X:        numpy array (1D) of emergency text messages\n",
    "    y:        numpy array (2D) of count matrix of labels (i.e. categories of text messages)\n",
    "    category: list of strings of category names\n",
    "    \n",
    "    '''\n",
    "    engine = create_engine('sqlite:///emergency_messages.db')\n",
    "    df = pd.read_sql_table('emergency_messages', con=engine)\n",
    "    \n",
    "    # define features and label arrays\n",
    "    X = df.message.values             # turn df column \"'message' into numpy.ndarray\n",
    "    y = df[df.columns[4:]].values     # turn df into numpy.ndarray\n",
    "    category = list(df.columns[4:])   # turn column names into list\n",
    "    \n",
    "    return X, y, category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    Input:\n",
    "    text: string. text containing a message.\n",
    "    \n",
    "    Output:\n",
    "    clean_tokens: list of strings. A list of strings containing normalized and lemmatized tokens.\n",
    "    \n",
    "    Workflow:\n",
    "    - Normalize text by converting to lowercase and removing punctuation\n",
    "    - Tokenize by splitting text up into words\n",
    "    - Remove Stop words\n",
    "    - Lemmatize to reduce words to the root or stem form\n",
    "    '''\n",
    "    \n",
    "    # Normalize text\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # initiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # lemmatize and remove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # iterate through each token\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        \n",
    "        # remove leading/trailing white space\n",
    "        clean_tok = tok.strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather update - a cold front from Cuba that could pass over Haiti\n",
      "['weather', 'update', 'cold', 'front', 'cuba', 'could', 'pas', 'haiti'] \n",
      "\n",
      "Is the Hurricane over or is it not over\n",
      "['hurricane'] \n",
      "\n",
      "Looking for someone but no name\n",
      "['looking', 'someone', 'name'] \n",
      "\n",
      "UN reports Leogane 80-90 destroyed. Only Hospital St. Croix functioning. Needs supplies desperately.\n",
      "['un', 'report', 'leogane', '80', '90', 'destroyed', 'hospital', 'st', 'croix', 'functioning', 'need', 'supply', 'desperately'] \n",
      "\n",
      "says: west side of Haiti, rest of the country today and tonight\n",
      "['say', 'west', 'side', 'haiti', 'rest', 'country', 'today', 'tonight'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test load function and tokenize function\n",
    "X, y, category = load_data()\n",
    "for message in X[:5]:\n",
    "    tokens = tokenize(message)\n",
    "    print(message)\n",
    "    print(tokens, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline():\n",
    "    '''\n",
    "    Text processing and machine learning pipeline.\n",
    "    Create a scikit-learn pipeline to output a ml model that predicts \n",
    "    a message classifications for 36 categories of emergency text messages (multi-output classification)\n",
    "    \n",
    "    Output: ml model with pipeline \n",
    "    '''\n",
    "    pipeline =  Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize)), # Convert a collection of text documents to a matrix of token \n",
    "            ('tfidf', TfidfTransformer()),          # Transform a count matrix to a normalized tf or tf-idf representation\n",
    "            ('clf', MultiOutputClassifier(RandomForestClassifier())) # Use the Random Forest algorithm\n",
    "        ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X, y, category = load_data()\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = model_pipeline()\n",
    "\n",
    "# train pipeline\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict classifiers on test data\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Accuracy` is defined as the percentage of correct predictions for the test data. It can be calculated by dividing the number of correct predictions by the number of total predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.240305149396\n"
     ]
    }
   ],
   "source": [
    "# Returns the mean accuracy on the given test data and label\n",
    "score = model.score(X_test, y_test, sample_weight=None)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `precision` is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "- The `recall` is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "- The `F-beta score` can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
    "\n",
    "- The F-beta score weights recall more than precision by a factor of beta. beta == 1.0 means recall and precision are equally important.\n",
    "\n",
    "- The `support` is the number of occurrences of each class in y_true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.85      0.92      0.88      6012\n",
      "               request       0.80      0.42      0.55      1313\n",
      "                 offer       0.00      0.00      0.00        36\n",
      "           aid_related       0.74      0.60      0.66      3255\n",
      "          medical_help       0.57      0.11      0.18       629\n",
      "      medical_products       0.73      0.06      0.11       406\n",
      "     search_and_rescue       0.78      0.03      0.07       202\n",
      "              security       0.00      0.00      0.00       151\n",
      "              military       0.59      0.07      0.12       244\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.91      0.29      0.44       493\n",
      "                  food       0.85      0.35      0.50       861\n",
      "               shelter       0.79      0.26      0.39       715\n",
      "              clothing       0.88      0.12      0.22       114\n",
      "                 money       0.43      0.03      0.06       174\n",
      "        missing_people       1.00      0.03      0.07        88\n",
      "              refugees       0.67      0.04      0.08       277\n",
      "                 death       0.83      0.14      0.24       373\n",
      "             other_aid       0.55      0.05      0.10      1034\n",
      "infrastructure_related       0.33      0.01      0.02       494\n",
      "             transport       0.55      0.07      0.12       346\n",
      "             buildings       0.65      0.13      0.22       403\n",
      "           electricity       0.81      0.08      0.14       168\n",
      "                 tools       0.00      0.00      0.00        51\n",
      "             hospitals       0.00      0.00      0.00        74\n",
      "                 shops       0.00      0.00      0.00        32\n",
      "           aid_centers       0.00      0.00      0.00        98\n",
      "  other_infrastructure       0.20      0.00      0.01       334\n",
      "       weather_related       0.84      0.61      0.70      2215\n",
      "                floods       0.88      0.30      0.45       663\n",
      "                 storm       0.78      0.36      0.49       735\n",
      "                  fire       0.67      0.02      0.05        83\n",
      "            earthquake       0.87      0.62      0.73       743\n",
      "                  cold       0.62      0.05      0.10       147\n",
      "         other_weather       0.61      0.05      0.09       402\n",
      "         direct_report       0.71      0.32      0.44      1522\n",
      "\n",
      "           avg / total       0.74      0.47      0.53     24887\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names = category, labels=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forst model has a high precision and a lower recall. Intuitively, this model shows a high ability of the classifier not to label as positive a sample that is negative. However, it shows a relatively low ability of the classifier to find all the positive samples.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7ffafd4ced90>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "               oob_score=False, random_state=None, verbose=0,\n",
       "               warm_start=False),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7ffafd4ced90>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'clf__estimator__n_jobs': 1,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show parameters for the pipline\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_model():\n",
    "    '''\n",
    "    Create a scikit-learn Pipeline with GridSearchCV to output a final model \n",
    "    that predicts a message classifications for the 36 categories (multi-output classification)\n",
    "    \n",
    "    use GridSearchCV to exhaustive search over specified parameter values for estimator\n",
    "    Output: cross-validation generator \n",
    "    '''\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(), n_jobs=-1)) # The number of jobs to run in parallel\n",
    "                                                                            # -1 means using all processors\n",
    "    ])\n",
    "\n",
    "    \n",
    "    parameters = {\n",
    "        'vect__max_features': (None, 5000),           # If not None, build a vocabulary that only consider the top \n",
    "                                                      # max_features ordered by term frequency across the corpus. \n",
    "                                                      # (default=None)\n",
    "                                                      \n",
    "        \n",
    "        'clf__estimator__n_estimators': [100,200]     # number of trees in the forest. The larger the better, \n",
    "                                                      # but also the longer it will take to compute. \n",
    "                                                      # In addition, note that results will stop getting \n",
    "                                                      # significantly better beyond a critical number of trees. \n",
    "                                                      # (default=100)\n",
    "    }\n",
    "    \n",
    "    # verbose = 3 to get the detailed realtime progress of the operation\n",
    "    # instantiate gridsearchcv having cv=2 to speed up computation (2 fold validation, by default it is set to 5)\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters,verbose = 3, cv=2, n_jobs=-1, scoring='f1_samples')\n",
    "\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "[CV] clf__estimator__n_estimators=100, vect__max_features=None .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=100, vect__max_features=None, score=0.5123915011810609, total= 5.2min\n",
      "[CV] clf__estimator__n_estimators=100, vect__max_features=None .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  6.3min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=100, vect__max_features=None, score=0.5112809758183677, total= 5.4min\n",
      "[CV] clf__estimator__n_estimators=100, vect__max_features=5000 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 12.8min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=100, vect__max_features=5000, score=0.5156499842031557, total= 4.2min\n",
      "[CV] clf__estimator__n_estimators=100, vect__max_features=5000 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=100, vect__max_features=5000, score=0.5133820418546913, total= 4.3min\n",
      "[CV] clf__estimator__n_estimators=200, vect__max_features=None .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=200, vect__max_features=None, score=0.5130345073645367, total= 8.8min\n",
      "[CV] clf__estimator__n_estimators=200, vect__max_features=None .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=200, vect__max_features=None, score=0.5132809662105281, total= 9.1min\n",
      "[CV] clf__estimator__n_estimators=200, vect__max_features=5000 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=200, vect__max_features=5000, score=0.5187480653672198, total= 6.8min\n",
      "[CV] clf__estimator__n_estimators=200, vect__max_features=5000 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=200, vect__max_features=5000, score=0.5162867616149125, total= 7.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed: 60.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=-1))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__max_features': (None, 5000), 'clf__estimator__n_estimators': [100, 200]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_samples', verbose=3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cv = find_model()\n",
    "model_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of GridSearchCV as table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__estimator__n_estimators</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>248.915445</td>\n",
       "      <td>4.446444</td>\n",
       "      <td>67.545905</td>\n",
       "      <td>0.494524</td>\n",
       "      <td>100</td>\n",
       "      <td>None</td>\n",
       "      <td>{'clf__estimator__n_estimators': 100, 'vect__m...</td>\n",
       "      <td>0.512392</td>\n",
       "      <td>0.511281</td>\n",
       "      <td>0.511836</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>4</td>\n",
       "      <td>0.768082</td>\n",
       "      <td>0.764686</td>\n",
       "      <td>0.766384</td>\n",
       "      <td>0.001698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191.310493</td>\n",
       "      <td>1.417540</td>\n",
       "      <td>63.046353</td>\n",
       "      <td>0.048306</td>\n",
       "      <td>100</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'clf__estimator__n_estimators': 100, 'vect__m...</td>\n",
       "      <td>0.515650</td>\n",
       "      <td>0.513382</td>\n",
       "      <td>0.514516</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>2</td>\n",
       "      <td>0.767403</td>\n",
       "      <td>0.764368</td>\n",
       "      <td>0.765886</td>\n",
       "      <td>0.001517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>450.412328</td>\n",
       "      <td>6.648899</td>\n",
       "      <td>87.324490</td>\n",
       "      <td>0.876009</td>\n",
       "      <td>200</td>\n",
       "      <td>None</td>\n",
       "      <td>{'clf__estimator__n_estimators': 200, 'vect__m...</td>\n",
       "      <td>0.513035</td>\n",
       "      <td>0.513281</td>\n",
       "      <td>0.513158</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>3</td>\n",
       "      <td>0.768114</td>\n",
       "      <td>0.764784</td>\n",
       "      <td>0.766449</td>\n",
       "      <td>0.001665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>335.679039</td>\n",
       "      <td>4.129806</td>\n",
       "      <td>79.109051</td>\n",
       "      <td>0.247143</td>\n",
       "      <td>200</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'clf__estimator__n_estimators': 200, 'vect__m...</td>\n",
       "      <td>0.518748</td>\n",
       "      <td>0.516287</td>\n",
       "      <td>0.517517</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>1</td>\n",
       "      <td>0.767328</td>\n",
       "      <td>0.764524</td>\n",
       "      <td>0.765926</td>\n",
       "      <td>0.001402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     248.915445      4.446444        67.545905        0.494524   \n",
       "1     191.310493      1.417540        63.046353        0.048306   \n",
       "2     450.412328      6.648899        87.324490        0.876009   \n",
       "3     335.679039      4.129806        79.109051        0.247143   \n",
       "\n",
       "  param_clf__estimator__n_estimators param_vect__max_features  \\\n",
       "0                                100                     None   \n",
       "1                                100                     5000   \n",
       "2                                200                     None   \n",
       "3                                200                     5000   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'clf__estimator__n_estimators': 100, 'vect__m...           0.512392   \n",
       "1  {'clf__estimator__n_estimators': 100, 'vect__m...           0.515650   \n",
       "2  {'clf__estimator__n_estimators': 200, 'vect__m...           0.513035   \n",
       "3  {'clf__estimator__n_estimators': 200, 'vect__m...           0.518748   \n",
       "\n",
       "   split1_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "0           0.511281         0.511836        0.000555                4   \n",
       "1           0.513382         0.514516        0.001134                2   \n",
       "2           0.513281         0.513158        0.000123                3   \n",
       "3           0.516287         0.517517        0.001231                1   \n",
       "\n",
       "   split0_train_score  split1_train_score  mean_train_score  std_train_score  \n",
       "0            0.768082            0.764686          0.766384         0.001698  \n",
       "1            0.767403            0.764368          0.765886         0.001517  \n",
       "2            0.768114            0.764784          0.766449         0.001665  \n",
       "3            0.767328            0.764524          0.765926         0.001402  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model_cv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__n_estimators': 200, 'vect__max_features': 5000}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best combination of parameters \n",
    "model_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuned = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize, max_features=5000)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(n_estimators=200),n_jobs=-1))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train pipeline\n",
    "model_tuned.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_tuned.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is defined as the percentage of correct predictions for the test data. It can be calculated by dividing the number of correct predictions by the number of total predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.281500317864\n"
     ]
    }
   ],
   "source": [
    "# Returns the mean accuracy on the given test data and label\n",
    "score = model_tuned.score(X_test, y_test, sample_weight=None)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.85      0.94      0.89      6012\n",
      "               request       0.82      0.50      0.62      1313\n",
      "                 offer       0.00      0.00      0.00        36\n",
      "           aid_related       0.74      0.70      0.72      3255\n",
      "          medical_help       0.63      0.14      0.23       629\n",
      "      medical_products       0.76      0.16      0.26       406\n",
      "     search_and_rescue       0.66      0.15      0.25       202\n",
      "              security       0.00      0.00      0.00       151\n",
      "              military       0.66      0.10      0.18       244\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.84      0.53      0.65       493\n",
      "                  food       0.85      0.70      0.77       861\n",
      "               shelter       0.80      0.47      0.59       715\n",
      "              clothing       0.75      0.13      0.22       114\n",
      "                 money       0.70      0.04      0.08       174\n",
      "        missing_people       1.00      0.02      0.04        88\n",
      "              refugees       0.75      0.08      0.14       277\n",
      "                 death       0.79      0.24      0.37       373\n",
      "             other_aid       0.58      0.06      0.10      1034\n",
      "infrastructure_related       0.43      0.01      0.01       494\n",
      "             transport       0.67      0.11      0.19       346\n",
      "             buildings       0.77      0.16      0.26       403\n",
      "           electricity       0.88      0.04      0.08       168\n",
      "                 tools       0.00      0.00      0.00        51\n",
      "             hospitals       0.00      0.00      0.00        74\n",
      "                 shops       0.00      0.00      0.00        32\n",
      "           aid_centers       0.00      0.00      0.00        98\n",
      "  other_infrastructure       0.17      0.00      0.01       334\n",
      "       weather_related       0.84      0.74      0.79      2215\n",
      "                floods       0.90      0.54      0.68       663\n",
      "                 storm       0.75      0.61      0.67       735\n",
      "                  fire       0.00      0.00      0.00        83\n",
      "            earthquake       0.88      0.82      0.85       743\n",
      "                  cold       0.76      0.17      0.28       147\n",
      "         other_weather       0.61      0.05      0.10       402\n",
      "         direct_report       0.76      0.36      0.49      1522\n",
      "\n",
      "           avg / total       0.76      0.56      0.60     24887\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names = category, labels=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuned Random Forsts model has significantly improved on recall compared to the untuned model (0.56 tuned model vs. 0.47 untuned model). Therefore, the tuned model's ability to find all the positive samples is higher. Furthermore, the tuned model shows a marginal improvement on precision as well (0.76 vs. 0.74). The F1-Score improved from 0.53 to 0.60. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try other machine learning algorithms AdaBoost developed by Yoav Freund and Robert Schapire (1999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_improved = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(AdaBoostClassifier(),n_jobs=-1))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train pipeline\n",
    "\n",
    "model_improved.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_improved.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.80      0.96      0.88      6012\n",
      "               request       0.77      0.51      0.62      1313\n",
      "                 offer       0.00      0.00      0.00        36\n",
      "           aid_related       0.76      0.59      0.67      3255\n",
      "          medical_help       0.59      0.25      0.35       629\n",
      "      medical_products       0.64      0.32      0.42       406\n",
      "     search_and_rescue       0.62      0.20      0.31       202\n",
      "              security       0.17      0.03      0.06       151\n",
      "              military       0.55      0.28      0.37       244\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.75      0.66      0.70       493\n",
      "                  food       0.81      0.66      0.73       861\n",
      "               shelter       0.74      0.55      0.63       715\n",
      "              clothing       0.70      0.43      0.53       114\n",
      "                 money       0.55      0.33      0.41       174\n",
      "        missing_people       0.68      0.19      0.30        88\n",
      "              refugees       0.58      0.22      0.32       277\n",
      "                 death       0.77      0.40      0.53       373\n",
      "             other_aid       0.50      0.15      0.23      1034\n",
      "infrastructure_related       0.36      0.10      0.15       494\n",
      "             transport       0.66      0.18      0.29       346\n",
      "             buildings       0.72      0.42      0.53       403\n",
      "           electricity       0.58      0.18      0.27       168\n",
      "                 tools       0.00      0.00      0.00        51\n",
      "             hospitals       0.26      0.08      0.12        74\n",
      "                 shops       0.00      0.00      0.00        32\n",
      "           aid_centers       0.22      0.04      0.07        98\n",
      "  other_infrastructure       0.29      0.08      0.12       334\n",
      "       weather_related       0.86      0.68      0.76      2215\n",
      "                floods       0.86      0.56      0.68       663\n",
      "                 storm       0.77      0.54      0.63       735\n",
      "                  fire       0.61      0.17      0.26        83\n",
      "            earthquake       0.88      0.79      0.83       743\n",
      "                  cold       0.66      0.33      0.44       147\n",
      "         other_weather       0.46      0.13      0.20       402\n",
      "         direct_report       0.70      0.39      0.50      1522\n",
      "\n",
      "           avg / total       0.73      0.58      0.62     24887\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names = category, labels=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The avg. F1-score of 0.62 for the model using the AdaBoost algorith is higher than agv. F1-score of 0.59 for the tuned model using the Random Forest algorithm. Therefore, we should keep the AdaBoost model.\n",
    "\n",
    "Let us try to find the best set of parameters for the AdaBoost model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use grid search to find better parameters for the AdaBoost model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_AdaBoost_model():\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(AdaBoostClassifier(), n_jobs=-1)) # The number of jobs to run in parallel\n",
    "                                                                            # -1 means using all processors\n",
    "    ])\n",
    "\n",
    "    \n",
    "    parameters = {\n",
    "        'vect__max_features': (None, 5000),          # If not None, build a vocabulary that only consider the top \n",
    "                                                      # max_features ordered by term frequency across the corpus. \n",
    "                                                      # (default=None)\n",
    "                                                      \n",
    "        \n",
    "        'clf__estimator__n_estimators': [50,100]     # n_estimatorsint, default=50 \n",
    "                                                      \n",
    "    }\n",
    "\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters,verbose = 3, n_jobs=-1, scoring='f1_samples')\n",
    "\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_features=None ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=50, vect__max_features=None, score=0.5314074872066465, total= 2.7min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_features=None ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.8min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=50, vect__max_features=None, score=0.5264283478394459, total= 2.7min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_features=None ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  7.7min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=50, vect__max_features=None, score=0.5347441217956864, total= 2.7min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_features=5000 ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=50, vect__max_features=5000, score=0.5327994911122119, total= 2.5min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_features=5000 ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=50, vect__max_features=5000, score=0.5252559895730625, total= 2.5min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_features=5000 ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=50, vect__max_features=5000, score=0.5362578179558622, total= 2.5min\n",
      "[CV] clf__estimator__n_estimators=100, vect__max_features=None .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=100, vect__max_features=None, score=0.5260752668940597, total= 3.8min\n",
      "[CV] clf__estimator__n_estimators=100, vect__max_features=None .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=100, vect__max_features=None, score=0.5194421055431541, total= 3.8min\n",
      "[CV] clf__estimator__n_estimators=100, vect__max_features=None .......\n"
     ]
    }
   ],
   "source": [
    "best_model = find_AdaBoost_model()\n",
    "\n",
    "# train pipeline\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of GridSearchCV as table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(best_model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best combination of parameters \n",
    "best_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    '''\n",
    "    Create a tuned scikit-learn pipeline using the AdaBoost algorithm \n",
    "    to output a ml model that predicts a message classifications for 36 categories \n",
    "    of emergency text messages (multi-output classification)\n",
    "    \n",
    "    Output: ml model with pipeline \n",
    "    '''\n",
    "    model = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize, max_features=5000)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(AdaBoostClassifier(n_estimators=50), n_jobs=-1)) \n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "# train pipeline\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names = category, labels=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The avg. F1-score of 0.64 for the tuned AdaBoost model is higher than the F1-score of 0.63 for the untuned model using the AdaBoost algorith and it is higher than agv. F1-score of 0.59 for the tuned model using the Random Forest algorithm. Therefore, we should keep the tuned AdaBoost model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pickle file in the current working directory\n",
    "pkl_filename = \"pickle_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
